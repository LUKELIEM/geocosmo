{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earthquake Catalog\n",
    "==================\n",
    "\n",
    "The following cells contain production code to:\n",
    "\n",
    "(1) process raw CSV files extracted from the ANSS website into CSV files with Python datetime and timestamp.  \n",
    "(2) filter earthquakes by:  \n",
    "  * Magnitude\n",
    "  * Latitudes and Longitudes (box coordinate - upper left; lower right)\n",
    "  * Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earthquake Catalog Extraction\n",
    "\n",
    "You need to go onto the ANSS website and performed queries to generate CSV files:  \n",
    "http://quake.geo.berkeley.edu/anss/catalog-search.html  \n",
    "\n",
    "The code assumes you will be following the following naming convention:\n",
    "\n",
    "alleq_mag5.csv - all earthquakes worldwide of magnitude > 5.0  \n",
    "alleq_mag4.csv - all earthquakes worldwide of magnitude > 4.0  \n",
    "alleq_mag3.csv - all earthquakes worldwide of magnitude > 3.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Conversion\n",
    "\n",
    "The code below takes in earthquake data from a CVS file and do the following:  \n",
    "* Convert date-time into Unix Timestamp and add an extra column\n",
    "* Store each row of earthquake data into a dictionary with 5 fields - (1) date-time, (2) timestamp, (3) long, (4) lat, (5) magnitude, and (6) depth  \n",
    "* Write the dictionary to a new CVS file\n",
    "\n",
    "The new CSV files generated follow the following naming convention:\n",
    "\n",
    "mag3_eq_dict.csv  \n",
    "mag4_eq_dict.csv  \n",
    "mag5_eq_dict.csv  \n",
    "\n",
    "Note that we will need to perform some **manual** corrections due to some bad data in the ANSS dataset (hours is 32, or second is 60 and so on). For earthquake data of magnitude 3.0 and above, there are only 5 such errors so it is not worth writing code to catch and clean these errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from data_util.data_analyze import *\n",
    "import calendar\n",
    "\n",
    "earthquakes = []\n",
    "earthquake = {}\n",
    "\n",
    "input_filename = 'earthquake_data/alleq_mag5.csv'\n",
    "output_filename = 'earthquake_data/mag5_eq_dict.csv'\n",
    "\n",
    "# Read in earthquake data from CVS file\n",
    "with open(input_filename, 'rb') as f:\n",
    "    reader = csv.DictReader(f, delimiter=',')\n",
    "    fieldnames = reader.fieldnames\n",
    "    \n",
    "    for row in reader:\n",
    "        # Convert UTC datetime to UNIX timestamp\n",
    "        try:\n",
    "            dt = datetime.strptime(row['DateTime'], \"%Y/%m/%d %H:%M:%S.%f\") \n",
    "        except ValueError:\n",
    "            print row['DateTime']\n",
    "        timestamp = datatime_2_timestamp(dt,utc=True)  # I wrote this in data_analyze.py\n",
    "        \n",
    "        # Store data into a list of dictionary with 5 fields\n",
    "        earthquake[\"timestamp\"] = timestamp\n",
    "        earthquake[\"datetime\"] = row['DateTime']        \n",
    "        earthquake[\"lat\"] = float(row['Latitude'])\n",
    "        earthquake[\"long\"] = float(row['Longitude'])\n",
    "        try:\n",
    "            earthquake[\"depth\"] = float(row['Depth'])\n",
    "        except ValueError:\n",
    "            earthquake[\"depth\"] = np.nan\n",
    "            \n",
    "        earthquake[\"magnitude\"] = float(row['Magnitude']) \n",
    "        earthquakes.append(earthquake.copy())\n",
    "               \n",
    "f.close()\n",
    "\n",
    "# Write to a CVS file mapping the dictionaries to output rows.\n",
    "with open(output_filename, 'w') as csvfile:\n",
    "    fieldnames = ['timestamp', 'datetime', 'long', 'lat', 'magnitude', 'depth']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for quake in earthquakes:\n",
    "        writer.writerow({'timestamp':quake['timestamp'], 'datetime':quake['datetime'], 'long':quake[\"long\"], \n",
    "                         'lat':quake[\"lat\"], 'magnitude':quake['magnitude'], 'depth':quake['depth']})\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Time Frame\n",
    "\n",
    "I have written a function in ['data_util/eq_data_util.py'](data_util/eq_data_util.py) that extract earthquake data by time window and latitude/longitude box.\n",
    "\n",
    "This is very useful for generating truth labels for evaluating the efficacy of earthquake precursors or for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-10-25 06:45:00\n",
      "2016-07-31 00:00:00\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from data_util.eq_data_util import *\n",
    "\n",
    "\n",
    "input = 'earthquake_data/mag5_eq_dict.csv'\n",
    "output = 'earthquake_data/mag5_eq_kodiak_10-25-2014_07-31-2016.csv'\n",
    "\n",
    "begin = 1414219500  # 2014-10-25 06:45:00\n",
    "end = 1469923200  # 2016-07-31 00:00:00\n",
    "Alaska_Kodiak_Window = ((62.2,-158.3),(52.2,-148.3))  # Kodiak Alaska is at 57.2 lat, -153.3 long\n",
    "\n",
    "num_eq = extract_eq_data(input, output, timewindow=(1414219500,1469923200), latlong=Alaska_Kodiak_Window)\n",
    "print datetime.utcfromtimestamp(1414219500)\n",
    "print datetime.utcfromtimestamp(1469923200)\n",
    "print num_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
