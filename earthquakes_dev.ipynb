{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earthquake Catalog\n",
    "==================\n",
    "\n",
    "I have gone on the ANSS website and performed queries to generate 3 CSV files:  \n",
    "http://quake.geo.berkeley.edu/anss/catalog-search.html  \n",
    "\n",
    "alleq_mag5.csv - all earthquakes worldwide of magnitude > 5.0  \n",
    "alleq_mag4.csv - all earthquakes worldwide of magnitude > 4.0  \n",
    "alleq_mag3.csv - all earthquakes worldwide of magnitude > 3.0  \n",
    "\n",
    "In this Notebook, we will develop a couple of functions to do the following:\n",
    "* Print out some sample data\n",
    "* Convert time (from datetime to UNIX timestamp)\n",
    "* Filter earthquake data by:\n",
    "  * Magnitude\n",
    "  * Latitudes and Longitudes (box coordinate - upper left; lower right)\n",
    "  * Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count earthquakes\n",
    "I have written a function in data_analyze.py to count the number of rows in a cvs file.\n",
    "\n",
    "* alleq_mag5.csv - 84,838 earthquake datapoints\n",
    "* alleq_mag4.csv - 420,078 earthquake datapoints\n",
    "* alleq_mag3.csv - 627,484 earthquake datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627484\n"
     ]
    }
   ],
   "source": [
    "from data_util.data_analyze import *\n",
    "\n",
    "num = num_datapoints('earthquake_data/alleq_mag3.csv', dict=True)  # The data has header\n",
    "print num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 items per row of data\n",
      "['DateTime,Latitude,Longitude,Depth,Magnitude,MagType,NbStations,Gap,Distance,RMS,Source,EventID']\n",
      "['1898/06/29', '18:36:00.00,52.0000,172.0000,0.00,7.60,ML,0,,,,AK,']\n",
      "['1898/10/11', '16:37:32.70,50.7100,-179.5000,0.00,6.90,ML,0,,,,AK,']\n",
      "['1899/07/14', '13:32:00.00,60.0000,-150.0000,0.00,7.20,ML,0,,,,AK,']\n",
      "['1899/09/04', '00:22:00.00,60.0000,-142.0000,25.00,8.30,ML,0,,,,AK,']\n",
      "['1899/09/04', '04:40:00.00,60.0000,-142.0000,0.00,6.90,ML,0,,,,AK,']\n",
      "['1899/09/10', '17:25:00.00,60.0000,-140.0000,25.00,7.80,ML,0,,,,AK,']\n",
      "['1899/09/10', '21:40:00.00,60.0000,-140.0000,25.00,8.60,ML,0,,,,AK,']\n",
      "['1899/09/17', '12:50:00.00,59.0000,-136.0000,0.00,6.90,ML,0,,,,AK,']\n",
      "['2016/12/30', '11:23:07.91,16.2416,-87.9808,21.25,4.40,Mb,,40,2,1.40,us,201612302024']\n",
      "['2016/12/30', '12:56:13.69,-30.5957,-177.7777,25.12,5.40,Mb,,59,8,0.96,us,201612302026']\n",
      "['2016/12/30', '13:11:20.78,-32.7096,-179.6688,10.00,4.80,Mb,,188,5,0.95,us,201612302027']\n",
      "['2016/12/30', '14:51:58.37,-30.6792,-178.7121,14.46,4.40,Mb,,260,9,0.82,us,201612302029']\n",
      "['2016/12/30', '15:01:05.48,9.2912,122.7357,69.42,4.10,Mb,,197,6,1.24,us,201612302031']\n",
      "['2016/12/30', '15:48:24.15,-31.5871,-68.6298,104.98,4.50,Mw,,34,0,1.40,us,201612302032']\n",
      "['2016/12/30', '17:13:22.43,52.8748,159.8018,56.61,4.90,Mb,,119,1,1.12,us,201612302034']\n",
      "['2016/12/30', '19:48:41.53,-5.3615,152.9771,27.23,5.00,Mb,,119,7,0.62,us,201612302035']\n",
      "['2016/12/30', '20:08:27.62,37.3973,141.4103,11.94,5.50,Mb,,97,1,0.91,us,201612302037']\n",
      "['2016/12/30', '21:08:34.00,13.3096,145.9751,10.00,4.90,Mb,,63,1,0.97,us,201612302039']\n",
      "['2016/12/30', '22:04:40.31,-32.9441,-179.8061,104.49,4.60,Mb,,157,5,0.52,us,201612302040']\n"
     ]
    }
   ],
   "source": [
    "# Run some setup code for this notebook.\n",
    "from data_util.data_analyze import *\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "file = 'earthquake_data/alleq_mag4.csv'\n",
    "size = num_datapoints(file, dict=True)  # The data has header\n",
    "\n",
    "with open(file, 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter=' ')\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        if i is 1:\n",
    "            print \"There are %d items per row of data\" %len(row)    \n",
    "\n",
    "        if i < 10:\n",
    "            print row   # Print first 10 rows\n",
    "            \n",
    "        if i > size-10:    \n",
    "            print row   # Print last 10 rows  \n",
    "            \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe earthquakes\n",
    "This function describes an earthquake event in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSS Mag 4+ Earthquakes\n",
      "['DateTime', 'Latitude', 'Longitude', 'Depth', 'Magnitude', 'MagType', 'NbStations', 'Gap', 'Distance', 'RMS', 'Source', 'EventID']\n",
      "The earthquake started on 1898/06/29 18:36:00.00\n",
      "Epicenter is located at 52.00 lat, 172.00 long\n",
      "Magnitude is 7.60, with depth of 0.00 \n",
      "\n",
      "The earthquake started on 1898/10/11 16:37:32.70\n",
      "Epicenter is located at 50.71 lat, -179.50 long\n",
      "Magnitude is 6.90, with depth of 0.00 \n",
      "\n",
      "The earthquake started on 1899/07/14 13:32:00.00\n",
      "Epicenter is located at 60.00 lat, -150.00 long\n",
      "Magnitude is 7.20, with depth of 0.00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('earthquake_data/alleq_mag4.csv', 'rb') as f:\n",
    "    reader = csv.DictReader(f, delimiter=',')\n",
    "    fieldnames = reader.fieldnames\n",
    "\n",
    "    print 'ANSS Mag 4+ Earthquakes'\n",
    "    print fieldnames\n",
    "\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        if i > 3:\n",
    "            break\n",
    "\n",
    "        print \"The earthquake started on %s\" % row['DateTime']\n",
    "        print \"Epicenter is located at %.2f lat, %.2f long\" %(float(row['Latitude']), float(row['Longitude']))\n",
    "        print \"Magnitude is %.2f, with depth of %.2f \\n\" %(float(row['Magnitude']), float(row['Depth']))\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of earthquakes based on magnitudes:\n",
    "\n",
    "Mag 8+ -   \n",
    "Mag 7+ -  \n",
    "Mag 6+ -   \n",
    "Mag 5+ - 84,838  \n",
    "Mag 4+ - 420,078  \n",
    "Mag 3+ - 627,484  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627484\n"
     ]
    }
   ],
   "source": [
    "from data_util.data_conversion import *\n",
    "\n",
    "num = num_datapoints('earthquake_data/alleq_mag3.csv', dict=True)  # The data has header\n",
    "print num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTC Datetime --> UNIX Timestamp\n",
    "\n",
    "It is rather cumbersome to work with 6 seperate columns of data denoting year, month, date, hour, minute and second. So we convert it to Unix timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1493596800\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import calendar\n",
    "\n",
    "dt = datetime.datetime(2017, 5, 1, 0, 0, 0)\n",
    "timestamp = calendar.timegm(dt.timetuple())\n",
    "print timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Dictionary\n",
    "The code below takes in earthquake data from a CVS file and do the following:  \n",
    "* Convert date-time into Unix Timestamp and add an extra column\n",
    "* Store each row of earthquake data into a dictionary with 5 fields - (1) date-time, (2) timestamp, (3) long, (4) lat, (5) magnitude, and (6) depth  \n",
    "* Write the dictionary to a new CVS file\n",
    "\n",
    "The CSV files generated are mag3_eq_dict.csv, mag4_eq_dict.csv and mag5_eq_dict.csv. Note that we need to perform some **manual** corrections due to bad data in the ANSS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from data_util.data_analyze import *\n",
    "import calendar\n",
    "\n",
    "earthquakes = []\n",
    "earthquake = {}\n",
    "\n",
    "# Read in earthquake data from CVS file\n",
    "with open('earthquake_data/alleq_mag3.csv', 'rb') as f:\n",
    "    reader = csv.DictReader(f, delimiter=',')\n",
    "    fieldnames = reader.fieldnames\n",
    "    \n",
    "    for row in reader:\n",
    "        # Convert UTC datetime to UNIX timestamp\n",
    "        try:\n",
    "            dt = datetime.strptime(row['DateTime'], \"%Y/%m/%d %H:%M:%S.%f\") \n",
    "        except ValueError:\n",
    "            print row['DateTime']\n",
    "        timestamp = datatime_2_timestamp(dt,utc=True)  # I wrote this in data_analyze.py\n",
    "        \n",
    "        # Store data into a list of dictionary with 5 fields\n",
    "        earthquake[\"timestamp\"] = timestamp\n",
    "        earthquake[\"datetime\"] = row['DateTime']        \n",
    "        earthquake[\"lat\"] = float(row['Latitude'])\n",
    "        earthquake[\"long\"] = float(row['Longitude'])\n",
    "        try:\n",
    "            earthquake[\"depth\"] = float(row['Depth'])\n",
    "        except ValueError:\n",
    "            earthquake[\"depth\"] = np.nan\n",
    "            \n",
    "        earthquake[\"magnitude\"] = float(row['Magnitude']) \n",
    "        earthquakes.append(earthquake.copy())\n",
    "               \n",
    "f.close()\n",
    "\n",
    "# Write to a CVS file mapping the dictionaries to output rows.\n",
    "with open('earthquake_data/mag3_eq_dict.csv', 'w') as csvfile:\n",
    "    fieldnames = ['timestamp', 'datetime', 'long', 'lat', 'magnitude', 'depth']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for quake in earthquakes:\n",
    "        writer.writerow({'timestamp':quake['timestamp'], 'datetime':quake['datetime'], 'long':quake[\"long\"], \n",
    "                         'lat':quake[\"lat\"], 'magnitude':quake['magnitude'], 'depth':quake['depth']})\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Time Frame\n",
    "\n",
    "I have written a function in ['data_util/eq_data_util.py'](data_util/eq_data_util.py) that extract earthquake data by time window and latitude/longitude box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2714\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from data_util.eq_data_util import *\n",
    "\n",
    "input = 'earthquake_data/mag5_eq_dict.csv'\n",
    "output = 'earthquake_data/mag5_eq_timeframe.csv'\n",
    "\n",
    "begin = 1414219500  # 2014-10-25 06:45:00\n",
    "end = 1469923200  # 2016-07-31 00:00:00\n",
    "\n",
    "num_eq = extract_eq_data(input, output, timewindow=(1414219500,1469923200), latlong=((90,180),(-90,-180)))\n",
    "\n",
    "print num_eq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Earthquakes for Alaska Kodiak station 7\n",
    "\n",
    "I have written a function in ['data_util/eq_data_util.py'](data_util/eq_data_util.py) that extract earthquake data by time window and latitude/longitude box.\n",
    "\n",
    "* Number of magnitude 4+ earthquakes occurring in the lat/long box defined by ((59.2,-155.3),(55.2,-151.3)) from 2014-12-03 to 2016-07-31. There are 11 such events.\n",
    "\n",
    "* Number of magnitude 5+ earthquakes occurring in the lat/long box defined by ((59.2,-155.3),(55.2,-151.3)) from 2014-12-03 to 2016-07-31. There are zero such events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-12-03 05:40:00\n",
      "2016-07-31 00:00:00\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from data_util.eq_data_util import *\n",
    "\n",
    "input = 'earthquake_data/mag5_eq_dict.csv'\n",
    "output = 'earthquake_data/mag5_eq_Kodiak_20141203_20160731.csv'\n",
    "\n",
    "Alaska_Kodiak = ((59.2,-155.3),(55.2,-151.3))  # Kodiak Alaska is at 57.2 lat, -153.3 long\n",
    "num_eq = extract_eq_data(input, output, timewindow=(1417585200,1469923200), latlong=Alaska_Kodiak)\n",
    "\n",
    "print datetime.utcfromtimestamp(1417585200)\n",
    "print datetime.utcfromtimestamp(1469923200)\n",
    "print num_eq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mag 5+ Quakes - Wider Window\n",
    "\n",
    "we then extract earthquakes occurring in a wider lat/long box defined by ((62.2,-158.3),(52.2,-148.3)) from 2014-12-03 to 2016-07-31.\n",
    "\n",
    "* Number of magnitude 4+:  There are 100 such events.\n",
    "\n",
    "* Number of magnitude 5+: There are 8 such events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-12-03 05:40:00\n",
      "2016-07-31 00:00:00\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from data_util.eq_data_util import *\n",
    "\n",
    "input = 'earthquake_data/mag4_eq_dict.csv'\n",
    "output = 'earthquake_data/mag4_eq_Kodiak_20141203_20160731_wider2.csv'\n",
    "\n",
    "Alaska_Kodiak = ((62.2,-158.3),(52.2,-148.3))  # Kodiak Alaska is at 57.2 lat, -153.3 long\n",
    "num_eq = extract_eq_data(input, output, timewindow=(1417585200,1469923200), latlong=Alaska_Kodiak)\n",
    "\n",
    "print datetime.utcfromtimestamp(1417585200)\n",
    "print datetime.utcfromtimestamp(1469923200)\n",
    "print num_eq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mag 4+ Quakes - Wider Window\n",
    "\n",
    "we then extract all magnitude 4+ earthquakes occurring in the lat/long box defined by ((62.2,-158.3),(52.2,-148.3)) from 2014-12-03 to 2016-07-31. There are 100 such events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-12-03 05:40:00\n",
      "2016-07-31 00:00:00\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from data_util.eq_data_util import *\n",
    "\n",
    "input = 'earthquake_data/mag4_eq_dict.csv'\n",
    "output = 'earthquake_data/mag4_eq_Kodiak_20141203_20160731_wider2.csv'\n",
    "\n",
    "Alaska_Kodiak = ((62.2,-158.3),(52.2,-148.3))  # Kodiak Alaska is at 57.2 lat, -153.3 long\n",
    "num_eq = extract_eq_data(input, output, timewindow=(1417585200,1469923200), latlong=Alaska_Kodiak)\n",
    "\n",
    "print datetime.utcfromtimestamp(1417585200)\n",
    "print datetime.utcfromtimestamp(1469923200)\n",
    "print num_eq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIX Timestamp --> Datatime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-10 00:10:08\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from data_util.eq_data_util import *\n",
    "\n",
    "print datetime.utcfromtimestamp(1449706208)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
